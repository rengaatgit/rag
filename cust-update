# main_script.py
import os
import json
import requests
from typing import Any, List, Optional, Dict, Sequence, Generator, AsyncGenerator # Added Generator, AsyncGenerator

from llama_index.core.embeddings.base import BaseEmbedding, Embedding
from llama_index.core.llms.custom import CustomLLM
from llama_index.core.llms.llm import LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.callbacks import CallbackManager
from llama_index.core.llms.base import CompletionResponse # Added import

from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# --- Configuration ---
# Replace with your Databricks details
# It's recommended to use environment variables for sensitive data like tokens
DATABRICKS_HOST = os.getenv("DATABRICKS_HOST", "https://your-databricks-workspace.cloud.databricks.com")
DATABRICKS_TOKEN = os.getenv("DATABRICKS_TOKEN", "YOUR_DATABRICKS_API_TOKEN")
# Replace with your specific model serving endpoint paths
EMBEDDING_ENDPOINT_PATH = "/serving-endpoints/your-embedding-model/invocations"
LLM_ENDPOINT_PATH = "/serving-endpoints/your-llm-model/invocations"

# ChromaDB configuration
CHROMA_DB_PATH = "./chroma_db_git_logs"
CHROMA_COLLECTION_NAME = "git_log_commits"

# Path to your git log JSON file
GIT_LOG_JSON_FILE = "git_log_data.json" # Ensure this file exists and is structured as expected

# --- 1. Custom Databricks Embedding Class ---
class DatabricksEmbedding(BaseEmbedding):
    """
    Custom LlamaIndex embedding class to use Databricks model serving for embeddings.
    """
    def __init__(
        self,
        model_name: str = "databricks-embedding-model", # Or your specific model identifier
        databricks_host: str = DATABRICKS_HOST,
        endpoint_path: str = EMBEDDING_ENDPOINT_PATH,
        api_token: str = DATABRICKS_TOKEN,
        embed_batch_size: int = 16, # Adjust based on your model endpoint's capabilities
        callback_manager: Optional[CallbackManager] = None,
    ):
        super().__init__(
            model_name=model_name,
            embed_batch_size=embed_batch_size,
            callback_manager=callback_manager,
        )
        self._endpoint_url = f"{databricks_host}{endpoint_path}"
        self._api_token = api_token
        if not self._api_token or self._api_token == "YOUR_DATABRICKS_API_TOKEN":
            raise ValueError("DATABRICKS_TOKEN is not set. Please provide a valid API token.")
        if "your-databricks-workspace" in self._endpoint_url:
            raise ValueError("DATABRICKS_HOST is not set correctly. Please provide your workspace URL.")


    def _get_headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self._api_token}",
            "Content-Type": "application/json",
        }

    def _call_databricks_embedding_api(self, texts: List[str]) -> List[List[float]]:
        """
        Makes an API call to the Databricks embedding endpoint.
        Adjust the payload structure based on your endpoint's expected input format.
        """
        payload = {"inputs": texts} 

        try:
            response = requests.post(self._endpoint_url, headers=self._get_headers(), json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            if "predictions" in result and isinstance(result["predictions"], list):
                return result["predictions"]
            elif "embeddings" in result and isinstance(result["embeddings"], list):
                 return result["embeddings"]
            else:
                print(f"Unexpected response structure from embedding endpoint: {result}")
                raise ValueError("Failed to parse embeddings from Databricks endpoint response.")

        except requests.exceptions.RequestException as e:
            print(f"Error calling Databricks embedding endpoint: {e}")
            return [[] for _ in texts] 
        except (ValueError, KeyError) as e:
            print(f"Error processing response from Databricks embedding endpoint: {e}")
            return [[] for _ in texts]


    def _get_text_embedding(self, text: str) -> List[float]:
        """Get text embedding for a single text."""
        embeddings = self._call_databricks_embedding_api([text])
        return embeddings[0] if embeddings and embeddings[0] else []

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get text embeddings for a list of texts."""
        return self._call_databricks_embedding_api(texts)

    async def _aget_text_embedding(self, text: str) -> List[float]:
        return self._get_text_embedding(text)

    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        return self._get_text_embeddings(texts)

# --- 2. Custom Databricks LLM Class ---
class DatabricksLLM(CustomLLM):
    """
    Custom LlamaIndex LLM class to use Databricks model serving for language models.
    """
    context_window: int = 4096
    num_output: int = 256
    model_name: str = "databricks-llm"

    def __init__(
        self,
        model_name: str = "databricks-llm",
        databricks_host: str = DATABRICKS_HOST,
        endpoint_path: str = LLM_ENDPOINT_PATH,
        api_token: str = DATABRICKS_TOKEN,
        context_window: int = 4096,
        num_output: int = 256,
        callback_manager: Optional[CallbackManager] = None,
        **kwargs: Any,
    ):
        super().__init__(callback_manager=callback_manager, **kwargs)
        self._endpoint_url = f"{databricks_host}{endpoint_path}"
        self._api_token = api_token
        self.model_name = model_name
        self.context_window = context_window
        self.num_output = num_output
        
        if not self._api_token or self._api_token == "YOUR_DATABRICKS_API_TOKEN":
            raise ValueError("DATABRICKS_TOKEN is not set. Please provide a valid API token.")
        if "your-databricks-workspace" in self._endpoint_url:
            raise ValueError("DATABRICKS_HOST is not set correctly. Please provide your workspace URL.")


    def _get_headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self._api_token}",
            "Content-Type": "application/json",
        }

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse: # Return CompletionResponse
        """Synchronous completion."""
        payload = {
            "messages": [{"role": "user", "content": prompt}], 
            "max_tokens": self.num_output,
        }
        completion_text = "" # Initialize completion_text
        try:
            response = requests.post(self._endpoint_url, headers=self._get_headers(), json=payload, timeout=180)
            response.raise_for_status()
            result = response.json()
            raw_response_data = result # Store raw response for potential use in CompletionResponse

            if "predictions" in result and isinstance(result["predictions"], list) and len(result["predictions"]) > 0:
                if isinstance(result["predictions"][0], str):
                     completion_text = result["predictions"][0]
                elif isinstance(result["predictions"][0], dict) and "text" in result["predictions"][0]:
                     completion_text = result["predictions"][0]["text"]
                elif isinstance(result["predictions"][0], dict) and "generated_text" in result["predictions"][0]: 
                     completion_text = result["predictions"][0]["generated_text"]
                else:
                    error_msg = f"Unexpected prediction format in: {result['predictions'][0]}"
                    print(error_msg)
                    return CompletionResponse(text=f"Error: {error_msg}", raw=raw_response_data)
            elif "choices" in result and isinstance(result["choices"], list) and result["choices"]:
                completion_text = result["choices"][0].get("text", "") 
                if not completion_text and "message" in result["choices"][0] and "content" in result["choices"][0]["message"]: 
                    completion_text = result["choices"][0]["message"]["content"]
                if not completion_text: # If still no text
                    error_msg = f"Could not extract text from choices: {result['choices'][0]}"
                    print(error_msg)
                    return CompletionResponse(text=f"Error: {error_msg}", raw=raw_response_data)

            elif "outputs" in result and isinstance(result["outputs"], list) and result["outputs"]:
                 completion_text = result["outputs"][0].get("generated_text", "")
                 if not completion_text:
                    error_msg = f"Could not extract text from outputs: {result['outputs'][0]}"
                    print(error_msg)
                    return CompletionResponse(text=f"Error: {error_msg}", raw=raw_response_data)
            else:
                error_msg = f"Unexpected response structure from LLM endpoint: {result}"
                print(error_msg)
                return CompletionResponse(text=f"Error: {error_msg}", raw=raw_response_data)
            
            return CompletionResponse(text=completion_text, raw=raw_response_data)

        except requests.exceptions.RequestException as e:
            print(f"Error calling Databricks LLM endpoint: {e}")
            return CompletionResponse(text=f"Error: Could not get response from LLM. Details: {e}", raw={"error": str(e)})
        except (ValueError, KeyError) as e:
            print(f"Error processing response from Databricks LLM endpoint: {e}")
            return CompletionResponse(text=f"Error: Could not parse response from LLM. Details: {e}", raw={"error": str(e)})


    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> Generator[CompletionResponse, None, None]:
        """Streaming completion (falls back to non-streaming complete)."""
        print("Streaming not implemented for DatabricksLLM, falling back to complete and yielding a single response.")
        completion_obj = self.complete(prompt, **kwargs) # This now returns a CompletionResponse object
        yield completion_obj # Yield the single CompletionResponse object

    # Removed the overridden completion_to_prompt method.
    # It will now inherit from the LLM base class:
    # def completion_to_prompt(self, completion_response: CompletionResponse) -> str:
    #     return completion_response.text or ""

    async def achat_complete(self, messages: Sequence[Any], **kwargs: Any) -> Any: # ChatMessage
        raise NotImplementedError("Async chat completion not implemented for this custom LLM.")

    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """Async completion (currently wraps sync version)."""
        # For true async, use an async HTTP client (e.g., httpx)
        print("Warning: DatabricksLLM.acomplete is using synchronous 'complete' method.")
        return self.complete(prompt, **kwargs) # Returns CompletionResponse

    async def astream_complete(self, prompt: str, **kwargs: Any) -> AsyncGenerator[CompletionResponse, None]:
        """Async streaming completion (currently wraps sync version)."""
        # For true async, implement async HTTP streaming.
        print("Warning: DatabricksLLM.astream_complete is using synchronous 'stream_complete' method.")
        # This makes astream_complete an async generator if stream_complete is a sync generator
        for chunk in self.stream_complete(prompt, **kwargs): # stream_complete yields CompletionResponse
            yield chunk
            
# --- 3. Process Git Log JSON ---
def load_git_log_documents(file_path: str) -> List[Document]:
    """
    Loads git log data from a JSON file and converts entries to LlamaIndex Documents.
    """
    documents = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            git_logs = json.load(f)
    except FileNotFoundError:
        print(f"Error: Git log JSON file not found at {file_path}")
        print("Please create a dummy git_log_data.json file with the expected structure if you want to run this.")
        print("""
        Example git_log_data.json:
        [
          {
            "commit_hash": "a1b2c3d4",
            "author": "User One <user1@example.com>",
            "date": "2024-05-10T10:00:00Z",
            "message": "Fix critical bug #123 in payment module\\n\\nThis commit addresses a severe issue where payments were failing under high load. The fix involves optimizing the database query and adding better error handling.\\n\\n src/payment/processor.py | 10 +++++-----\\n src/utils/db.py        |  5 +++++\\n 2 files changed, 10 insertions(+), 5 deletions(-)",
            "files_changed": ["src/payment/processor.py", "src/utils/db.py"]
          },
          {
            "commit_hash": "e5f6g7h8",
            "author": "User Two <user2@example.com>",
            "date": "2024-05-09T15:30:00Z",
            "message": "Add new feature: User Profile Export\\n\\nImplemented functionality for users to export their profile data in JSON format. Includes all user settings and activity history.\\n\\n src/user/profile.py    | 50 ++++++++++++++++++++++++++++++++++++++++++++++++\\n src/api/endpoints.py | 12 +++++++++++\\n tests/user/test_profile.py | 30 +++++++++++++++++++++++++++++\\n 3 files changed, 92 insertions(+)",
            "files_changed": ["src/user/profile.py", "src/api/endpoints.py", "tests/user/test_profile.py"]
          }
        ]
        """)
        return [] 

    for entry in git_logs:
        text_content = entry.get("message", "") 
        metadata = {
            "commit_hash": entry.get("commit_hash", ""),
            "author": entry.get("author", ""),
            "date": entry.get("date", ""), 
            "files_changed": entry.get("files_changed", []) 
        }
        metadata = {k: (str(v) if v is not None else "") for k, v in metadata.items()}
        
        if not text_content.strip():
            text_content = f"Commit {entry.get('commit_hash', 'N/A')} by {entry.get('author', 'N/A')} with no message."

        doc = Document(text=text_content, metadata=metadata)
        documents.append(doc)
    
    print(f"Loaded {len(documents)} documents from {file_path}")
    return documents

# --- Main Execution ---
if __name__ == "__main__":
    print("Starting RAG pipeline with Databricks and ChromaDB...")

    if not os.path.exists(GIT_LOG_JSON_FILE):
        print(f"'{GIT_LOG_JSON_FILE}' not found. Creating a dummy file for demonstration.")
        dummy_data = [
          {
            "commit_hash": "a1b2c3d4",
            "author": "User One <user1@example.com>",
            "date": "2024-05-10T10:00:00Z",
            "message": "Fix critical bug #123 in payment module\n\nThis commit addresses a severe issue where payments were failing under high load. The fix involves optimizing the database query and adding better error handling.\n\n src/payment/processor.py | 10 +++++-----\n src/utils/db.py        |  5 +++++\n 2 files changed, 10 insertions(+), 5 deletions(-)",
            "files_changed": ["src/payment/processor.py", "src/utils/db.py"]
          },
          {
            "commit_hash": "e5f6g7h8",
            "author": "User Two <user2@example.com>",
            "date": "2024-05-09T15:30:00Z",
            "message": "Add new feature: User Profile Export\n\nImplemented functionality for users to export their profile data in JSON format. Includes all user settings and activity history.\n\n src/user/profile.py    | 50 ++++++++++++++++++++++++++++++++++++++++++++++++\n src/api/endpoints.py | 12 +++++++++++\n tests/user/test_profile.py | 30 +++++++++++++++++++++++++++++\n 3 files changed, 92 insertions(+)",
            "files_changed": ["src/user/profile.py", "src/api/endpoints.py", "tests/user/test_profile.py"]
          }
        ]
        with open(GIT_LOG_JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(dummy_data, f, indent=2)

    print("Initializing Databricks embedding model and LLM...")
    try:
        db_embedding_model = DatabricksEmbedding()
        db_llm = DatabricksLLM()
    except ValueError as e:
        print(f"Configuration error: {e}")
        print("Please ensure DATABRICKS_HOST and DATABRICKS_TOKEN are correctly set as environment variables or in the script.")
        exit(1)

    Settings.llm = db_llm
    Settings.embed_model = db_embedding_model
    Settings.chunk_size = 512
    Settings.chunk_overlap = 20

    print("Models initialized and set in LlamaIndex Settings.")

    print(f"Loading documents from '{GIT_LOG_JSON_FILE}'...")
    documents = load_git_log_documents(GIT_LOG_JSON_FILE)

    if not documents:
        print("No documents loaded. Exiting.")
        exit(1)

    print(f"Setting up ChromaDB at path: '{CHROMA_DB_PATH}' with collection: '{CHROMA_COLLECTION_NAME}'")
    db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    chroma_collection = db_client.get_or_create_collection(CHROMA_COLLECTION_NAME)
    print(f"ChromaDB collection '{CHROMA_COLLECTION_NAME}' ready. Entry count: {chroma_collection.count()}")

    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    print("Indexing documents... This may take a while.")
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
    )
    print(f"Indexing complete. ChromaDB collection '{CHROMA_COLLECTION_NAME}' new entry count: {chroma_collection.count()}")

    print("Setting up query engine...")
    query_engine = index.as_query_engine(
        similarity_top_k=3
    )
    print("Query engine ready.")

    user_query = "What were the major bug fixes related to the payment module?"
    print(f"\nUser Query: {user_query}")

    response = query_engine.query(user_query)

    print("\nLLM Response:")
    # The 'response' attribute of the LlamaIndex Response object holds the string form of the LLM's answer.
    # This remains correct even if db_llm.complete() returns a CompletionResponse object.
    print(response.response) 

    print("\nRetrieved Source Nodes (for context):")
    for i, node in enumerate(response.source_nodes):
        print(f"\n--- Source Node {i+1} (Score: {node.score:.4f}) ---")
        print(f"Commit Hash: {node.metadata.get('commit_hash', 'N/A')}")
        print(f"Author: {node.metadata.get('author', 'N/A')}")
        print(f"Date: {node.metadata.get('date', 'N/A')}")
        print("Content snippet:")
        print(node.get_content()[:500] + "...") 

    print("\n--- RAG Pipeline Demonstration Complete ---")

