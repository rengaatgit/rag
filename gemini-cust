!pip install llama-index llama-index-core chromadb llama-index-vector-stores-chroma
# main_script.py
import os
import json
import requests
from typing import Any, List, Optional, Dict, Sequence
from llama_index.core.base.embeddings.base import BaseEmbedding, Embedding
from llama_index.core.llms.custom import CustomLLM
from llama_index.core.base.llms.types import LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.callbacks import CallbackManager

from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# --- Configuration ---
# Replace with your Databricks details
# It's recommended to use environment variables for sensitive data like tokens
DATABRICKS_HOST = os.getenv("DATABRICKS_HOST", "https://your-databricks-workspace.cloud.databricks.com")
DATABRICKS_TOKEN = os.getenv("DATABRICKS_TOKEN", "YOUR_DATABRICKS_API_TOKEN")
# Replace with your specific model serving endpoint paths
EMBEDDING_ENDPOINT_PATH = "/serving-endpoints/your-embedding-model/invocations"
LLM_ENDPOINT_PATH = "/serving-endpoints/your-llm-model/invocations"

# ChromaDB configuration
CHROMA_DB_PATH = "./chroma_db_git_logs"
CHROMA_COLLECTION_NAME = "git_log_commits"

# Path to your git log JSON file
GIT_LOG_JSON_FILE = "git_log_data.json" # Ensure this file exists and is structured as expected

# --- 1. Custom Databricks Embedding Class ---
class DatabricksEmbedding(BaseEmbedding):
    """
    Custom LlamaIndex embedding class to use Databricks model serving for embeddings.
    """
    def __init__(
        self,
        model_name: str = "databricks-embedding-model", # Or your specific model identifier
        databricks_host: str = DATABRICKS_HOST,
        endpoint_path: str = EMBEDDING_ENDPOINT_PATH,
        api_token: str = DATABRICKS_TOKEN,
        embed_batch_size: int = 16, # Adjust based on your model endpoint's capabilities
        callback_manager: Optional[CallbackManager] = None,
    ):
        super().__init__(
            model_name=model_name,
            embed_batch_size=embed_batch_size,
            callback_manager=callback_manager,
        )
        self._endpoint_url = f"{databricks_host}{endpoint_path}"
        self._api_token = api_token
        if not self._api_token or self._api_token == "YOUR_DATABRICKS_API_TOKEN1":
            raise ValueError("DATABRICKS_TOKEN is not set. Please provide a valid API token.")
        if "your-databricks-workspace1" in self._endpoint_url:
            raise ValueError("DATABRICKS_HOST is not set correctly. Please provide your workspace URL.")


    def _get_headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self._api_token}",
            "Content-Type": "application/json",
        }

    def _call_databricks_embedding_api(self, texts: List[str]) -> List[List[float]]:
        """
        Makes an API call to the Databricks embedding endpoint.
        Adjust the payload structure based on your endpoint's expected input format.
        Example payload structures:
        - {"inputs": texts}
        - {"dataframe_records": [{"text": t} for t in texts]}
        - {"instances": texts}
        """
        # This is an example payload structure.
        # YOU MUST MODIFY THIS to match your Databricks embedding model server's expected input.
        payload = {"inputs": texts} # Common for sentence-transformers or similar models

        try:
            response = requests.post(self._endpoint_url, headers=self._get_headers(), json=payload, timeout=60)
            response.raise_for_status()  # Raise an exception for HTTP errors
            
            # YOU MUST MODIFY THIS to parse the response from your Databricks endpoint.
            # Common response structures:
            # - {"predictions": [[0.1, ...], [0.2, ...]]}
            # - {"embeddings": [[0.1, ...], [0.2, ...]]}
            result = response.json()
            if "predictions" in result and isinstance(result["predictions"], list):
                return result["predictions"]
            elif "embeddings" in result and isinstance(result["embeddings"], list):
                 return result["embeddings"]
            else:
                # Add more sophisticated error handling or logging here
                print(f"Unexpected response structure from embedding endpoint: {result}")
                raise ValueError("Failed to parse embeddings from Databricks endpoint response.")

        except requests.exceptions.RequestException as e:
            print(f"Error calling Databricks embedding endpoint: {e}")
            # Depending on the error, you might want to return empty embeddings or re-raise
            # For simplicity, returning empty lists of the correct shape if texts were processed
            return [[] for _ in texts] # Or handle more gracefully
        except (ValueError, KeyError) as e:
            print(f"Error processing response from Databricks embedding endpoint: {e}")
            return [[] for _ in texts]


    def _get_text_embedding(self, text: str) -> List[float]:
        """Get text embedding for a single text."""
        embeddings = self._call_databricks_embedding_api([text])
        return embeddings[0] if embeddings and embeddings[0] else []

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get text embeddings for a list of texts."""
        return self._call_databricks_embedding_api(texts)

    async def _aget_text_embedding(self, text: str) -> List[float]:
        # For simplicity, calling the sync version. Implement true async if needed.
        return self._get_text_embedding(text)

    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        # For simplicity, calling the sync version. Implement true async if needed.
        return self._get_text_embeddings(texts)

    # Adding the missing methods to resolve the TypeError
    def _get_query_embedding(self, query: str) -> List[float]:
        """Get query embedding for a single query."""
        return self._get_text_embedding(query)  # You can reuse the text embedding logic for queries

    async def _aget_query_embedding(self, query: str) -> List[float]:
        """Get query embedding for a single query asynchronously."""
        return await self._aget_text_embedding(query)  # You can reuse the async text embedding logic for queries


# --- 2. Custom Databricks LLM Class ---
class DatabricksLLM(CustomLLM):
    """
    Custom LlamaIndex LLM class to use Databricks model serving for language models.
    """
    context_window: int = 4096  # Adjust based on your model
    num_output: int = 256      # Adjust based on your model and typical response length
    model_name: str = "databricks-llm" # Or your specific model identifier
    # System prompt can be set here if your model supports it and it's static
    # system_prompt: str = "You are a helpful assistant." 

    def __init__(
        self,
        model_name: str = "databricks-llm",
        databricks_host: str = DATABRICKS_HOST,
        endpoint_path: str = LLM_ENDPOINT_PATH,
        api_token: str = DATABRICKS_TOKEN,
        context_window: int = 4096,
        num_output: int = 256,
        callback_manager: Optional[CallbackManager] = None,
        **kwargs: Any,
    ):
        super().__init__(callback_manager=callback_manager, **kwargs)
        self._endpoint_url = f"{databricks_host}{endpoint_path}"
        self._api_token = api_token
        self.model_name = model_name
        self.context_window = context_window
        self.num_output = num_output
        
        if not self._api_token or self._api_token == "YOUR_DATABRICKS_API_TOKEN1":
            raise ValueError("DATABRICKS_TOKEN is not set. Please provide a valid API token.")
        if "your-databricks-workspace1" in self._endpoint_url:
            raise ValueError("DATABRICKS_HOST is not set correctly. Please provide your workspace URL.")


    def _get_headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self._api_token}",
            "Content-Type": "application/json",
        }

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
            # is_chat_model=True # Set to True if your endpoint expects chat message format
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> str: # Changed from CompletionResponse to str
        """Synchronous completion."""
        # YOU MUST MODIFY THIS payload to match your Databricks LLM server's expected input.
        # Common payload structures:
        # - {"prompt": prompt, "max_tokens": self.num_output}
        # - {"messages": [{"role": "user", "content": prompt}], "max_tokens": self.num_output}
        # - {"dataframe_records": [{"prompt": prompt}], "params": {"max_tokens": self.num_output}}
        payload = {
            "messages": [{"role": "user", "content": prompt}], # Example for a chat-like model
            "max_tokens": self.num_output,
            # Add other parameters like temperature, top_p as needed by your endpoint
            # "temperature": 0.7 
        }

        try:
            response = requests.post(self._endpoint_url, headers=self._get_headers(), json=payload, timeout=180)
            response.raise_for_status()
            
            # YOU MUST MODIFY THIS to parse the response from your Databricks LLM endpoint.
            # Common response structures:
            # - {"predictions": ["response text"]}
            # - {"choices": [{"text": "response text"}]}
            # - {"outputs": [{"generated_text": "response text"}]}
            result = response.json()

            # Example parsing, adjust as needed:
            if "predictions" in result and isinstance(result["predictions"], list) and len(result["predictions"]) > 0:
                # If predictions is a list of strings
                if isinstance(result["predictions"][0], str):
                     completion_text = result["predictions"][0]
                # If predictions is a list of dicts, e.g. for MLflow pyfunc format
                elif isinstance(result["predictions"][0], dict) and "text" in result["predictions"][0]:
                     completion_text = result["predictions"][0]["text"]
                elif isinstance(result["predictions"][0], dict) and "generated_text" in result["predictions"][0]: # HuggingFace-like
                     completion_text = result["predictions"][0]["generated_text"]
                else:
                    raise ValueError(f"Unexpected prediction format in: {result['predictions'][0]}")
            elif "choices" in result and isinstance(result["choices"], list) and result["choices"]:
                completion_text = result["choices"][0].get("text", "") # OpenAI-like
                if not completion_text and "message" in result["choices"][0] and "content" in result["choices"][0]["message"]: # Chat completion
                    completion_text = result["choices"][0]["message"]["content"]

            elif "outputs" in result and isinstance(result["outputs"], list) and result["outputs"]: # Another common pattern
                 completion_text = result["outputs"][0].get("generated_text", "")
            else:
                print(f"Unexpected response structure from LLM endpoint: {result}")
                raise ValueError("Failed to parse completion from Databricks LLM endpoint response.")
            
            return completion_text # Return the string directly

        except requests.exceptions.RequestException as e:
            print(f"Error calling Databricks LLM endpoint: {e}")
            return f"Error: Could not get response from LLM. Details: {e}"
        except (ValueError, KeyError) as e:
            print(f"Error processing response from Databricks LLM endpoint: {e}")
            return f"Error: Could not parse response from LLM. Details: {e}"


    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> Any: # Changed from CompletionResponse to Any
        """Streaming completion (Not implemented for this example)."""
        # If your Databricks endpoint supports streaming, implement that logic here.
        # For now, it falls back to non-streaming completion.
        # To implement streaming, you would typically use `requests.post(..., stream=True)`
        # and iterate over `response.iter_lines()` or `response.iter_content()`.
        # Each chunk would be a `CompletionResponse` with `delta` set.
        print("Streaming not implemented for DatabricksLLM, falling back to complete.")
        full_response_text = self.complete(prompt, **kwargs)
        
        # Simulate streaming by yielding the full response as a single delta
        # This is a basic placeholder. True streaming requires more complex handling.
        # For LlamaIndex, you'd yield `CompletionResponse(text=full_response_text, delta=full_response_text)`
        # or `ChatResponse` with `ChatMessage` and `delta`.
        # For simplicity, this example's CustomLLM returns str from complete,
        # so stream_complete would need to be adapted if used by a streaming-specific component.
        # This part might need adjustment based on how LlamaIndex handles CustomLLM stream_complete return.
        # A common pattern is to yield `str` chunks.
        yield full_response_text


    # Required by LlamaIndex CustomLLM
    def completion_to_prompt(self, completion: str) -> str: # Changed input type from CompletionResponse
        return completion # If 'completion' is already the string response

    # Required by LlamaIndex CustomLLM
    async def achat_complete(self, messages: Sequence[Any], **kwargs: Any) -> Any: # ChatMessage
        raise NotImplementedError("Async chat completion not implemented for this custom LLM.")

    # Required by LlamaIndex CustomLLM
    async def acomplete(self, prompt: str, **kwargs: Any) -> str: # CompletionResponse
        # For simplicity, calling the sync version. Implement true async if needed.
        return self.complete(prompt, **kwargs)

    # Required by LlamaIndex CustomLLM
    async def astream_complete(self, prompt: str, **kwargs: Any) -> Any: # CompletionResponse
        # For simplicity, calling the sync version. Implement true async if needed.
        # This is a generator, so it needs to be handled carefully for async.
        # A proper async generator would use `async for`.
        for chunk in self.stream_complete(prompt, **kwargs):
            yield chunk
            
# --- 3. Process Git Log JSON ---
def load_git_log_documents(file_path: str) -> List[Document]:
    """
    Loads git log data from a JSON file and converts entries to LlamaIndex Documents.
    Assumes a JSON file containing a list of commit objects.
    Example structure for each commit object in the JSON:
    {
      "commit_hash": "a1b2c3d4",
      "author": "User One <user1@example.com>",
      "date": "2024-05-10T10:00:00Z", // ISO format date
      "message": "Fix critical bug #123\\n\\nDetailed explanation...\\n\\n src/file1.py | 2 +-\\n...",
      "files_changed": ["src/file1.py"] // Optional
    }
    """
    documents = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            git_logs = json.load(f)
    except FileNotFoundError:
        print(f"Error: Git log JSON file not found at {file_path}")
        print("Please create a dummy git_log_data.json file with the expected structure if you want to run this.")
        print("""
        Example git_log_data.json:
        [
          {
            "commit_hash": "a1b2c3d4",
            "author": "User One <user1@example.com>",
            "date": "2024-05-10T10:00:00Z",
            "message": "Fix critical bug #123 in payment module\\n\\nThis commit addresses a severe issue where payments were failing under high load. The fix involves optimizing the database query and adding better error handling.\\n\\n src/payment/processor.py | 10 +++++-----\\n src/utils/db.py        |  5 +++++\\n 2 files changed, 10 insertions(+), 5 deletions(-)",
            "files_changed": ["src/payment/processor.py", "src/utils/db.py"]
          },
          {
            "commit_hash": "e5f6g7h8",
            "author": "User Two <user2@example.com>",
            "date": "2024-05-09T15:30:00Z",
            "message": "Add new feature: User Profile Export\\n\\nImplemented functionality for users to export their profile data in JSON format. Includes all user settings and activity history.\\n\\n src/user/profile.py    | 50 ++++++++++++++++++++++++++++++++++++++++++++++++\\n src/api/endpoints.py | 12 +++++++++++\\n tests/user/test_profile.py | 30 +++++++++++++++++++++++++++++\\n 3 files changed, 92 insertions(+)",
            "files_changed": ["src/user/profile.py", "src/api/endpoints.py", "tests/user/test_profile.py"]
          }
        ]
        """)
        return [] # Return empty list if file not found

    for entry in git_logs:
        text_content = entry.get("message", "") # The main content for embedding and retrieval
        metadata = {
            "commit_hash": entry.get("commit_hash", ""),
            "author": entry.get("author", ""),
            "date": entry.get("date", ""), # Store date for potential time-based filtering later
            "files_changed": entry.get("files_changed", []) # List of files
        }
        # Ensure all metadata values are strings or simple types compatible with ChromaDB
        metadata = {k: (str(v) if v is not None else "") for k, v in metadata.items()}
        
        # LlamaIndex Document requires text. If message is empty, skip or use a placeholder.
        if not text_content.strip():
            text_content = f"Commit {entry.get('commit_hash', 'N/A')} by {entry.get('author', 'N/A')} with no message."

        doc = Document(text=text_content, metadata=metadata)
        documents.append(doc)
    
    print(f"Loaded {len(documents)} documents from {file_path}")
    return documents

# --- Main Execution ---
if __name__ == "__main__":
    print("Starting RAG pipeline with Databricks and ChromaDB...")

    # --- Step 0: Create a dummy git_log_data.json if it doesn't exist for testing ---
    if not os.path.exists(GIT_LOG_JSON_FILE):
        print(f"'{GIT_LOG_JSON_FILE}' not found. Creating a dummy file for demonstration.")
        dummy_data = [
          {
            "commit_hash": "a1b2c3d4",
            "author": "User One <user1@example.com>",
            "date": "2024-05-10T10:00:00Z",
            "message": "Fix critical bug #123 in payment module\n\nThis commit addresses a severe issue where payments were failing under high load. The fix involves optimizing the database query and adding better error handling.\n\n src/payment/processor.py | 10 +++++-----\n src/utils/db.py        |  5 +++++\n 2 files changed, 10 insertions(+), 5 deletions(-)",
            "files_changed": ["src/payment/processor.py", "src/utils/db.py"]
          },
          {
            "commit_hash": "e5f6g7h8",
            "author": "User Two <user2@example.com>",
            "date": "2024-05-09T15:30:00Z",
            "message": "Add new feature: User Profile Export\n\nImplemented functionality for users to export their profile data in JSON format. Includes all user settings and activity history.\n\n src/user/profile.py    | 50 ++++++++++++++++++++++++++++++++++++++++++++++++\n src/api/endpoints.py | 12 +++++++++++\n tests/user/test_profile.py | 30 +++++++++++++++++++++++++++++\n 3 files changed, 92 insertions(+)",
            "files_changed": ["src/user/profile.py", "src/api/endpoints.py", "tests/user/test_profile.py"]
          }
        ]
        with open(GIT_LOG_JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(dummy_data, f, indent=2)

    # --- Step 1 & 2: Initialize Custom Models ---
    print("Initializing Databricks embedding model and LLM...")
    try:
        db_embedding_model = DatabricksEmbedding()
        db_llm = DatabricksLLM()
    except ValueError as e:
        print(f"Configuration error: {e}")
        print("Please ensure DATABRICKS_HOST and DATABRICKS_TOKEN are correctly set as environment variables or in the script.")
        exit(1)

    # --- Step 2.5: Set models in LlamaIndex Settings (for LlamaIndex 0.10.x+) ---
    # This makes the LLM and embedding model globally available to LlamaIndex components
    # or you can pass them directly to index/query_engine constructors.
    Settings.llm = db_llm
    Settings.embed_model = db_embedding_model
    Settings.chunk_size = 512  # Optional: configure chunk size for document processing
    Settings.chunk_overlap = 20 # Optional: configure chunk overlap

    print("Models initialized and set in LlamaIndex Settings.")

    # --- Step 3: Load and Process Git Log Data ---
    print(f"Loading documents from '{GIT_LOG_JSON_FILE}'...")
    documents = load_git_log_documents(GIT_LOG_JSON_FILE)

    if not documents:
        print("No documents loaded. Exiting.")
        exit(1)

    # --- Step 4: Setup ChromaDB Vector Store ---
    print(f"Setting up ChromaDB at path: '{CHROMA_DB_PATH}' with collection: '{CHROMA_COLLECTION_NAME}'")
    # Initialize ChromaDB client. 
    # For a persistent local DB:
    db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    # If your ChromaDB 1.0.8 is hosted and requires HttpClient:
    # db_client = chromadb.HttpClient(host="your_chroma_host", port="your_chroma_port") 
    
    # Get or create the collection
    chroma_collection = db_client.get_or_create_collection(CHROMA_COLLECTION_NAME)
    print(f"ChromaDB collection '{CHROMA_COLLECTION_NAME}' ready. Entry count: {chroma_collection.count()}")

    # Create LlamaIndex ChromaVectorStore
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    # Create StorageContext
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # --- Step 5: Index Documents ---
    # This will embed the documents using your custom DatabricksEmbedding model
    # and store them in ChromaDB via the StorageContext.
    print("Indexing documents... This may take a while depending on the number of documents and embedding model speed.")
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        # embed_model is taken from Settings if not passed explicitly
        # show_progress=True # Uncomment for progress bar with many documents
    )
    print(f"Indexing complete. ChromaDB collection '{CHROMA_COLLECTION_NAME}' new entry count: {chroma_collection.count()}")

    # --- Step 6: Query the Embeddings and Feed to LLM ---
    print("Setting up query engine...")
    # The query engine will use the LLM from Settings
    query_engine = index.as_query_engine(
        similarity_top_k=3 # Retrieve top 3 most similar documents
        # You can customize other retrieval parameters here
    )
    print("Query engine ready.")

    # Example query
    user_query = "What were the major bug fixes related to the payment module?"
    print(f"\nUser Query: {user_query}")

    # Perform the query. This involves:
    # 1. Embedding the user_query using CustomDatabricksEmbedding.
    # 2. Retrieving relevant document chunks from ChromaDB.
    # 3. Augmenting the prompt with retrieved context.
    # 4. Sending the augmented prompt to CustomDatabricksLLM.
    response = query_engine.query(user_query)

    print("\nLLM Response:")
    print(response.response) # Accessing the 'response' attribute for the text

    print("\nRetrieved Source Nodes (for context):")
    for i, node in enumerate(response.source_nodes):
        print(f"\n--- Source Node {i+1} (Score: {node.score:.4f}) ---")
        print(f"Commit Hash: {node.metadata.get('commit_hash', 'N/A')}")
        print(f"Author: {node.metadata.get('author', 'N/A')}")
        print(f"Date: {node.metadata.get('date', 'N/A')}")
        print("Content snippet:")
        print(node.get_content()[:500] + "...") # Print a snippet of the node content

    print("\n--- RAG Pipeline Demonstration Complete ---")

    # Example of how to clear the collection if needed (be careful!)
    # print(f"Clearing all entries from collection '{CHROMA_COLLECTION_NAME}' for cleanup...")
    # db_client.delete_collection(CHROMA_COLLECTION_NAME)
    # print("Collection cleared.")
    # # Recreate for next run if needed
    # chroma_collection = db_client.get_or_create_collection(CHROMA_COLLECTION_NAME) 
    # print(f"Collection '{CHROMA_COLLECTION_NAME}' recreated and is empty: {chroma_collection.count()}")
